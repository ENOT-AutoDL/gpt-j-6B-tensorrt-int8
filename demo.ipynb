{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83e43b0-4a77-4e85-8ff4-59c0f6a70126",
   "metadata": {},
   "source": [
    "## Install all requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba79bed4-930e-4f75-bdd9-64c530695136",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: datasets==2.10.* in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.10.1)\n",
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: nvidia-ml-py in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (11.525.84)\n",
      "Requirement already satisfied: tensorrt==8.5.3.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (8.5.3.1)\n",
      "Requirement already satisfied: torch==1.13.* in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.13.1+cu116)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (4.65.0)\n",
      "Requirement already satisfied: transformers==4.26.* in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (4.26.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./.venv/lib/python3.9/site-packages (from datasets==2.10.*->-r requirements.txt (line 1)) (2023.3.0)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.9/site-packages (from datasets==2.10.*->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in ./.venv/lib/python3.9/site-packages (from datasets==2.10.*->-r requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from datasets==2.10.*->-r requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from datasets==2.10.*->-r requirements.txt (line 1)) (0.3.6)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (from datasets==2.10.*->-r requirements.txt (line 1)) (1.5.3)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.9/site-packages (from datasets==2.10.*->-r requirements.txt (line 1)) (3.8.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from datasets==2.10.*->-r requirements.txt (line 1)) (1.24.2)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.9/site-packages (from datasets==2.10.*->-r requirements.txt (line 1)) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from datasets==2.10.*->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in ./.venv/lib/python3.9/site-packages (from datasets==2.10.*->-r requirements.txt (line 1)) (11.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.9/site-packages (from datasets==2.10.*->-r requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11 in ./.venv/lib/python3.9/site-packages (from tensorrt==8.5.3.1->-r requirements.txt (line 4)) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11 in ./.venv/lib/python3.9/site-packages (from tensorrt==8.5.3.1->-r requirements.txt (line 4)) (8.8.1.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu11 in ./.venv/lib/python3.9/site-packages (from tensorrt==8.5.3.1->-r requirements.txt (line 4)) (11.11.3.6)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from torch==1.13.*->-r requirements.txt (line 6)) (4.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers==4.26.*->-r requirements.txt (line 8)) (2023.3.23)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from transformers==4.26.*->-r requirements.txt (line 8)) (3.10.7)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.9/site-packages (from transformers==4.26.*->-r requirements.txt (line 8)) (0.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.10.*->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.10.*->-r requirements.txt (line 1)) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.10.*->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.10.*->-r requirements.txt (line 1)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.10.*->-r requirements.txt (line 1)) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.10.*->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.10.*->-r requirements.txt (line 1)) (1.3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.10.*->-r requirements.txt (line 1)) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.10.*->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.10.*->-r requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.venv/lib/python3.9/site-packages (from pandas->datasets==2.10.*->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas->datasets==2.10.*->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.10.*->-r requirements.txt (line 1)) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d33cfd-15a2-4184-860d-549f72887655",
   "metadata": {},
   "source": [
    "## Define seq2seq model.\n",
    "### All important information with TensorRT initialization you can find in `utils/trt_model.py`.\n",
    "\n",
    "All our engines have 57 inputs and 57 outputs (1+2\\*n_attn_layers). First input is the list of tokens ids while other inputs are context. If you want to send empty context you should create empty tensor with the shape -> (1, 16, **0**, 256) for every context input. First output is the logits while other outputs are context. Our engines always return only new values of the context, so before send context to the next step you have to manually concatenate previous context values with new context values (see code snippet below).\n",
    "\n",
    "**Inputs names and shapes**:\n",
    "\n",
    "| | Name | Shape\n",
    "|---:|:-------------|:-----------|\n",
    "| 1 | `input_ids` | `(1, sequence_length)` |\n",
    "| 2 | `history_key_0` | `(1, 16, history_length, 256)` |\n",
    "| 3 | `history_value_0` | `(1, 16, history_length, 256)` |\n",
    "| 4 | `history_key_1` | `(1, 16, history_length, 256)` |\n",
    "| 5 | `history_value_1` | `(1, 16, history_length, 256)` |\n",
    "| ... |\n",
    "| 56 | `history_key_27` | `(1, 16, history_length, 256)` |\n",
    "| 57 | `history_value_27` | `(1, 16, history_length, 256)` |\n",
    "\n",
    "**Outputs names and shapes**:\n",
    "\n",
    "| | Name | Shape\n",
    "|---:|:-------------|:-----------|\n",
    "| 1 | `logits` | `(1, sequance_length, 50400)` |\n",
    "| 2 | `out_history_key_0` | `(1, 16, sequence_length, 256)` |\n",
    "| 3 | `out_history_value_0` | `(1, 16, sequence_length, 256)` |\n",
    "| 4 | `out_history_key_1` | `(1, 16, sequence_length, 256)` |\n",
    "| 5 | `out_history_value_1` | `(1, 16, sequence_length, 256)` |\n",
    "| ... |\n",
    "| 56 | `out_history_key_27` | `(1, 16, sequence_length, 256)` |\n",
    "| 57 | `out_history_value_27` | `(1, 16, sequence_length, 256)` |\n",
    "\n",
    "\n",
    "`sequence_length` - dynamic axis which value must be in the following range \\[1, 512\\]\n",
    "\n",
    "`history_length` - dynamic axis which value must be in the following range \\[0, 512\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca46628f-a940-4be0-9904-a5ee0ca161a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "from utils.trt_model import TrtModel\n",
    "\n",
    "\n",
    "class TrtSeq2SeqModel:\n",
    "    def __init__(self, path_to_engine: Union[Path, str]):\n",
    "        self._model = TrtModel(str(path_to_engine))\n",
    "\n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        return self._model.binding_shape('input_ids')[0]\n",
    "\n",
    "    def generate(self, input_ids: torch.Tensor, generate_len: int, return_logit: bool = False) -> torch.Tensor:\n",
    "        input_ids = input_ids.contiguous()\n",
    "\n",
    "        input_tensors = {'input_ids': input_ids}\n",
    "        for name in self._model.inputs:\n",
    "            if name.startswith('history') and 'profile' not in name:\n",
    "                # add empty context for the first run\n",
    "                input_tensors[name] = torch.empty(\n",
    "                    size=(self.batch_size, 16, 0, 256),\n",
    "                    dtype=self._model.binding_dtype(name),\n",
    "                    device='cuda',\n",
    "                )\n",
    "\n",
    "        result = []\n",
    "        output_tensors = None\n",
    "        for _ in range(generate_len):\n",
    "            output_tensors = self._model.run(input_tensors=input_tensors, output_tensors_cache=output_tensors)\n",
    "\n",
    "            logits = output_tensors['logits']\n",
    "            next_id = logits[:, -1, :].argmax(dim=-1, keepdims=True).to(torch.int32)\n",
    "            result.append(logits.clone() if return_logit else next_id)\n",
    "\n",
    "            # concatenate previous context values with new context values\n",
    "            input_tensors['input_ids'] = next_id\n",
    "            for name, new_value in output_tensors.items():\n",
    "                if name.startswith('out_history_') and 'profile' not in name:\n",
    "                    name = name[4:]\n",
    "                    input_tensors[name] = torch.cat((input_tensors[name], new_value), dim=-2)\n",
    "\n",
    "            model._model.context.set_optimization_profile_async(1)\n",
    "\n",
    "        dim = -2 if return_logit else -1\n",
    "        result = torch.cat(result, dim=dim)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6020183-cb7f-45d1-a67f-5b59bcc7f2ae",
   "metadata": {},
   "source": [
    "## Download prebuilt engine and initialize seq2seq model.\n",
    "\n",
    "**Currently you can find prebuilt engines only for next GPUs:**\n",
    "* RTX 2080 Ti\n",
    "* RTX 3080 Ti\n",
    "* RTX 4090\n",
    "\n",
    "**ONNX model and build script will be published later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fab8c5-2141-4d2a-8660-fa1d24c00502",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/31/2023-04:47:52] [TRT] [I] Loaded engine size: 11658 MiB\n",
      "[03/31/2023-04:47:55] [TRT] [V] Trying to load shared library libcudnn.so.8\n",
      "[03/31/2023-04:47:55] [TRT] [V] Loaded shared library libcudnn.so.8\n",
      "[03/31/2023-04:47:55] [TRT] [V] Using cuDNN as plugin tactic source\n",
      "[03/31/2023-04:47:56] [TRT] [V] Using cuDNN as core library tactic source\n",
      "[03/31/2023-04:47:56] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1593, GPU +428, now: CPU 16489, GPU 12658 (MiB)\n",
      "[03/31/2023-04:47:56] [TRT] [W] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "[03/31/2023-04:47:56] [TRT] [V] Deserialization required 3605501 microseconds.\n",
      "[03/31/2023-04:47:56] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +15, GPU +11619, now: CPU 15, GPU 11619 (MiB)\n",
      "[03/31/2023-04:47:56] [TRT] [V] Trying to load shared library libcudnn.so.8\n",
      "[03/31/2023-04:47:56] [TRT] [V] Loaded shared library libcudnn.so.8\n",
      "[03/31/2023-04:47:56] [TRT] [V] Using cuDNN as plugin tactic source\n",
      "[03/31/2023-04:47:56] [TRT] [V] Using cuDNN as core library tactic source\n",
      "[03/31/2023-04:47:56] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 4832, GPU 12658 (MiB)\n",
      "[03/31/2023-04:47:56] [TRT] [W] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "[03/31/2023-04:47:56] [TRT] [V] Total per-runner device persistent memory is 0\n",
      "[03/31/2023-04:47:56] [TRT] [V] Total per-runner host persistent memory is 624384\n",
      "[03/31/2023-04:47:56] [TRT] [V] Allocated activation device memory of size 422646784\n",
      "[03/31/2023-04:47:57] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +404, now: CPU 16, GPU 12023 (MiB)\n",
      "[03/31/2023-04:47:57] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n"
     ]
    }
   ],
   "source": [
    "from utils.engine import get_engine\n",
    "\n",
    "path_to_engine = '/4tb_first/kalgin/projects/gptj_e2e_example/out_single_onnx/2prof_fq_gptj_i8f32.engine' #get_engine()\n",
    "model = TrtSeq2SeqModel(path_to_engine=path_to_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d820349-975b-408b-a56b-011d0562c62a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8679a8f7-2aed-44ca-a4d2-a08e7fc9b68b",
   "metadata": {},
   "source": [
    "## Seq2seq example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a94b70-0756-4b44-aac2-68a6e843ae20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "set_optimization_profile_async(): incompatible function arguments. The following argument types are supported:\n    1. (self: tensorrt.tensorrt.IExecutionContext, profile_index: int, stream_handle: int) -> bool\n\nInvoked with: <tensorrt.tensorrt.IExecutionContext object at 0x7fe0fe7cf530>, 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m----> 9\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m (generated_ids,) \u001b[38;5;241m=\u001b[39m generated_ids\n\u001b[1;32m     11\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids)\n",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m, in \u001b[0;36mTrtSeq2SeqModel.generate\u001b[0;34m(self, input_ids, generate_len, return_logit)\u001b[0m\n\u001b[1;32m     43\u001b[0m             name \u001b[38;5;241m=\u001b[39m name[\u001b[38;5;241m4\u001b[39m:]\n\u001b[1;32m     44\u001b[0m             input_tensors[name] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((input_tensors[name], new_value), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_optimization_profile_async\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_logit \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     49\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(result, dim\u001b[38;5;241m=\u001b[39mdim)\n",
      "\u001b[0;31mTypeError\u001b[0m: set_optimization_profile_async(): incompatible function arguments. The following argument types are supported:\n    1. (self: tensorrt.tensorrt.IExecutionContext, profile_index: int, stream_handle: int) -> bool\n\nInvoked with: <tensorrt.tensorrt.IExecutionContext object at 0x7fe0fe7cf530>, 1"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-j-6B')\n",
    "\n",
    "input_text = 'Hello world!'\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors='pt')\n",
    "input_ids = input_ids['input_ids'].to(device='cuda', dtype=torch.int32)\n",
    "generated_ids = model.generate(input_ids, generate_len=100)\n",
    "(generated_ids,) = generated_ids\n",
    "generated_text = tokenizer.decode(generated_ids)\n",
    "\n",
    "print(input_text + generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e1ed1-b2b9-4f52-aa03-a1467fc396da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Accuracy validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a36706-4e3b-4c39-8e7b-fced32384cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset lambada (/home/igor_kalgin/.cache/huggingface/datasets/lambada/plain_text/1.1.0/9f7bada20233bfec7d1d888d179c81442d504fb3d0dd97cddeba020b19924373)\n",
      "Loading cached shuffled indices for dataset at /home/igor_kalgin/.cache/huggingface/datasets/lambada/plain_text/1.1.0/9f7bada20233bfec7d1d888d179c81442d504fb3d0dd97cddeba020b19924373/cache-9e3acb0e95629ff1.arrow\n",
      "Loading cached processed dataset at /home/igor_kalgin/.cache/huggingface/datasets/lambada/plain_text/1.1.0/9f7bada20233bfec7d1d888d179c81442d504fb3d0dd97cddeba020b19924373/cache-998a74ae77022345.arrow\n",
      "Acc = 78.85%: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4869/4869 [02:37<00:00, 30.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final acc = 78.85%\n"
     ]
    }
   ],
   "source": [
    "from utils.test import test_acc\n",
    "\n",
    "\n",
    "def predict_next_id(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    input_ids = input_ids.to(device='cuda', dtype=torch.int32)\n",
    "    result = model.generate(input_ids, generate_len=1)\n",
    "    result = result.detach().cpu()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "test_acc(predict_next_id, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a79e243-3be0-4815-a2ba-3a0b321f1aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from utils.test import test_acc\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path='EleutherAI/gpt-j-6B',\n",
    "    revision='main',\n",
    "    low_cpu_mem_usage=True,\n",
    ").eval()\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model = model.to('cuda') #, dtype=torch.float16)\n",
    "\n",
    "def predict_next_id(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    max_length = input_ids.shape[1] + 1\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids.cuda(), max_length=max_length, do_sample=False)\n",
    "        (output_ids,) = output_ids\n",
    "        \n",
    "    return output_ids[-1].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ccae99-527a-48f1-a8da-765547116571",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset lambada (/home/igor_kalgin/.cache/huggingface/datasets/lambada/plain_text/1.1.0/9f7bada20233bfec7d1d888d179c81442d504fb3d0dd97cddeba020b19924373)\n",
      "Loading cached shuffled indices for dataset at /home/igor_kalgin/.cache/huggingface/datasets/lambada/plain_text/1.1.0/9f7bada20233bfec7d1d888d179c81442d504fb3d0dd97cddeba020b19924373/cache-9e3acb0e95629ff1.arrow\n",
      "Loading cached processed dataset at /home/igor_kalgin/.cache/huggingface/datasets/lambada/plain_text/1.1.0/9f7bada20233bfec7d1d888d179c81442d504fb3d0dd97cddeba020b19924373/cache-998a74ae77022345.arrow\n",
      "  0%|                                                                                                                                                                            | 0/4869 [00:00<?, ?it/s]/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "  0%|                                                                                                                                                                            | 0/4869 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_next_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/utils/test.py:41\u001b[0m, in \u001b[0;36mtest_acc\u001b[0;34m(predict_next_id_function, verbose)\u001b[0m\n\u001b[1;32m     38\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     40\u001b[0m gt_id \u001b[38;5;241m=\u001b[39m input_ids[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 41\u001b[0m pred_id \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_next_id_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gt_id\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     44\u001b[0m hit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred_id \u001b[38;5;241m==\u001b[39m gt_id)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m, in \u001b[0;36mpredict_next_id\u001b[0;34m(input_ids)\u001b[0m\n\u001b[1;32m     14\u001b[0m max_length \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     (output_ids,) \u001b[38;5;241m=\u001b[39m output_ids\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/transformers/generation/utils.py:1391\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1386\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when doing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m greedy search.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1388\u001b[0m         )\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/transformers/generation/utils.py:2179\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2176\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2179\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2182\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2183\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2187\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/transformers/models/gptj/modeling_gptj.py:813\u001b[0m, in \u001b[0;36mGPTJForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    811\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 813\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    828\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/transformers/models/gptj/modeling_gptj.py:668\u001b[0m, in \u001b[0;36mGPTJModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    660\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    661\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    662\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m         head_mask[i],\n\u001b[1;32m    666\u001b[0m     )\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 668\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/transformers/models/gptj/modeling_gptj.py:302\u001b[0m, in \u001b[0;36mGPTJBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    300\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    301\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 302\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/transformers/models/gptj/modeling_gptj.py:203\u001b[0m, in \u001b[0;36mGPTJAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    192\u001b[0m     hidden_states: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m     Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Tuple[torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]],\n\u001b[1;32m    201\u001b[0m ]:\n\u001b[0;32m--> 203\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    205\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/4tb_first/kalgin/projects/demo-gpt-j-6B-tensorrt-int8/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "test_acc(predict_next_id, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd403453-4006-4c66-a4b9-171d3d71eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 79.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a975b-2ded-4483-8c9d-4e196367d858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
