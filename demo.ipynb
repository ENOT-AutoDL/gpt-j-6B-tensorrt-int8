{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83e43b0-4a77-4e85-8ff4-59c0f6a70126",
   "metadata": {},
   "source": [
    "## Install all requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba79bed4-930e-4f75-bdd9-64c530695136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d33cfd-15a2-4184-860d-549f72887655",
   "metadata": {},
   "source": [
    "## Define seq2seq model.\n",
    "### All important information with tensorrt initialization you can find in `utils/trt_model.py`.\n",
    "\n",
    "All our engines have 57 inputs and 57 outputs. First input is the list of tokens ids. All other inputs are context. If you want to send empty context, then you should create empty tensor with the shape -> (1, 16, **0**, 256) for every context input. First output is the logits. All other outputs are context. Our engines always return only new values of the context, so before send context to the next step you must manually concatenate previous context values with new context values (see example code below).\n",
    "\n",
    "Inputs names and shapes:\n",
    "<ol>\n",
    "    <li>input_ids (1, sequence_length)</li>\n",
    "    <li>history_key_0 (1, 16, history_length, 256)</li>\n",
    "    <li>history_value_0 (1, 16, history_length, 256)</li>\n",
    "    <li>history_key_1 (1, 16, history_length, 256)</li>\n",
    "    <li>history_value_1 (1, 16, history_length, 256)</li>\n",
    "</ol>\n",
    "...\n",
    "<ol start=\"56\">\n",
    "    <li>history_key_27 (1, 16, history_length, 256)</li>\n",
    "    <li>history_value_27 (1, 16, history_length, 256)</li>\n",
    "</ol>\n",
    "\n",
    "Outputs names and shapes:\n",
    "<ol>\n",
    "    <li>logits (1, sequance_length, 50400)</li>\n",
    "    <li>out_history_key_0 (1, 16, sequence_length, 256)</li>\n",
    "    <li>out_history_value_0 (1, 16, sequence_length, 256)</li>\n",
    "    <li>out_history_key_1 (1, 16, sequence_length, 256)</li>\n",
    "    <li>out_history_value_1 (1, 16, sequence_length, 256)</li>\n",
    "</ol>\n",
    "...\n",
    "<ol start=\"56\">\n",
    "    <li>out_history_key_27 (1, 16, sequence_length, 256)</li>\n",
    "    <li>out_history_value_27 (1, 16, sequence_length, 256)</li>\n",
    "</ol>\n",
    "\n",
    "`sequence_length` - dynamic axis which value must lie in the next range \\[1, 512\\]\n",
    "\n",
    "`history_length` - dynamic axis which value must lie in the next range \\[0, 512\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46628f-a940-4be0-9904-a5ee0ca161a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "from utils.trt_model import TrtModel\n",
    "\n",
    "\n",
    "class TrtSeq2SeqModel:\n",
    "    def __init__(self, path_to_engine: Union[Path, str]):\n",
    "        self._model = TrtModel(str(path_to_engine))\n",
    "\n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        return self._model.binding_shape('input_ids')[0]\n",
    "\n",
    "    def generate(self, input_ids: torch.Tensor, generate_len: int, return_logit: bool = False) -> torch.Tensor:\n",
    "        input_ids = input_ids.contiguous()\n",
    "\n",
    "        input_tensors = {'input_ids': input_ids}\n",
    "        for name in self._model.inputs:\n",
    "            if name.startswith('history'):\n",
    "                # add empty context for the first run\n",
    "                input_tensors[name] = torch.empty(\n",
    "                    size=(self.batch_size, 16, 0, 256),\n",
    "                    dtype=self._model.binding_dtype(name),\n",
    "                    device='cuda',\n",
    "                )\n",
    "\n",
    "        result = []\n",
    "        output_tensors = None\n",
    "        for i in range(generate_len):\n",
    "            output_tensors = self._model.run(input_tensors=input_tensors, output_tensors_cache=output_tensors)\n",
    "\n",
    "            logits = output_tensors['logits']\n",
    "            next_id = logits[:, -1, :].argmax(dim=-1, keepdims=True).to(torch.int32)\n",
    "            result.append(logits.clone() if return_logit else next_id)\n",
    "\n",
    "            # concatenate previous context values with new context values\n",
    "            input_tensors['input_ids'] = next_id\n",
    "            for name, new_value in output_tensors.items():\n",
    "                if name.startswith('out_history_'):\n",
    "                    name = name[4:]\n",
    "                    input_tensors[name] = torch.cat((input_tensors[name], new_value), dim=-2)\n",
    "\n",
    "        dim = -2 if return_logit else -1\n",
    "        result = torch.cat(result, dim=dim)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6020183-cb7f-45d1-a67f-5b59bcc7f2ae",
   "metadata": {},
   "source": [
    "## Download prebuilded engine and initialize seq2seq model.\n",
    "\n",
    "**Currently you can find prebuild engines only fo next GPUs:**\n",
    "* rtx2080ti\n",
    "* rtx3080ti\n",
    "* rtx4090\n",
    "\n",
    "**onnx + build script will be published later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fab8c5-2141-4d2a-8660-fa1d24c00502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.engine import get_engine\n",
    "\n",
    "path_to_engine = get_engine()\n",
    "model = TrtSeq2SeqModel(path_to_engine=path_to_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679a8f7-2aed-44ca-a4d2-a08e7fc9b68b",
   "metadata": {},
   "source": [
    "## Seq2seq example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a94b70-0756-4b44-aac2-68a6e843ae20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-j-6B')\n",
    "\n",
    "input_text = 'Hello world!'\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors='pt')\n",
    "input_ids = input_ids['input_ids'].to(device='cuda', dtype=torch.int32)\n",
    "generated_ids = model.generate(input_ids, generate_len=100)\n",
    "(generated_ids,) = generated_ids\n",
    "generated_text = tokenizer.decode(generated_ids)\n",
    "\n",
    "print(input_text + generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e1ed1-b2b9-4f52-aa03-a1467fc396da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Accuracy test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a36706-4e3b-4c39-8e7b-fced32384cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.test import test_acc\n",
    "\n",
    "\n",
    "def predict_last_id(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    input_ids = input_ids.to(device='cuda', dtype=torch.int32)\n",
    "    result = model.generate(input_ids, generate_len=1)\n",
    "    result = result.detach().cpu()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "test_acc(predict_last_id, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346ef4f-62ec-4ec0-b527-ae309b4620eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
